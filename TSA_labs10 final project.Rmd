---
title: "Time Series Analysis Spring 2024"
subtitle: " Forcasting Financial instruments price with VECM and ARIMA model"
author: "Anindita Basu, Vikram Bahadur"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE, 
                      cache   = FALSE,
                      message = FALSE, 
                      warning = FALSE)
options(scipen = 10)
```

## 1 Data importing

```{r}
library(tidyverse)
library(xts)
library(vars)
```
```{r cache = F}
library(xts)
library(lmtest)
library(tidyverse)
```

Let's also load the additional function:
```{r}
source("functions/testdf.R")
```

We will work with the Data concerning prices of financial instrument prices: y1, y2,y3,y4,y5,y6,y7,y8,y9,y10


Now, we have to import the data:
```{r}
TS2 <- read.csv("TSA_2024_project_data_1.csv")
```

The structure of the data:
```{r}
TS2 %>% glimpse()
head(TS2)
tail(TS2)
```

We have to correct the type of the `date` variable:
```{r}
TS2$date <- as.Date(TS2$date, format = "%Y-%m-%d")
TS2 %>% glimpse()
```

Let's also transform the `data.frame` into an `xts` object
```{r}
TS2 <- xts(TS2[, -1], order.by = TS2$date)
```

We will work 10 variables of y1,y2, y3,y4.......y10. Let's create their first differences.
```{r}
TS2$dy1 <- diff.xts(TS2$y1)
TS2$dy2 <- diff.xts(TS2$y2)
TS2$dy3 <- diff.xts(TS2$y3)
TS2$dy4 <- diff.xts(TS2$y4)
TS2$dy5 <- diff.xts(TS2$y5)
TS2$dy6 <- diff.xts(TS2$y6)
TS2$dy7 <- diff.xts(TS2$y7)
TS2$dy8 <- diff.xts(TS2$y8)
TS2$dy9 <- diff.xts(TS2$y9)
TS2$dy10 <- diff.xts(TS2$y10)
```

```{r}
head(TS2)
```





Plotting all variables on the graph
```{r}

 plot(TS2[, 1:10],
)
 
```


Difference
```{r}
 
 
 plot(TS2[, 11:20],
 
 )
```

## 2 Testing cointegration

In the next step, we will perform the tests of integration order.

First, let's apply the ADF test for the `y1` variable and its first differences.
```{r}
testdf(variable = TS2$y1,
       max.augmentations = 3)
testdf(variable = TS2$dy1, 
       max.augmentations = 3)
```

Since we can reject the null in the case of the first differences, we can conclude that the y1 is from I(1). 


let's apply the ADF test for the `y2` variable and its first differences

```{r}
testdf(variable = TS2$y2,
       max.augmentations = 3)
testdf(variable = TS2$dy2, 
       max.augmentations = 3)

```
```{r}
testdf(variable = TS2$y3,
       max.augmentations = 3)
testdf(variable = TS2$dy3, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y4,
       max.augmentations = 3)
testdf(variable = TS2$dy4, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y5,
       max.augmentations = 3)
testdf(variable = TS2$dy5, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y6,
       max.augmentations = 3)
testdf(variable = TS2$dy6, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y7,
       max.augmentations = 3)
testdf(variable = TS2$dy7, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y8,
       max.augmentations = 3)
testdf(variable = TS2$dy8, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y9,
       max.augmentations = 3)
testdf(variable = TS2$dy9, 
       max.augmentations = 3)
```
```{r}
testdf(variable = TS2$y10,
       max.augmentations = 3)
testdf(variable = TS2$dy10, 
       max.augmentations = 3)
```
Since we can reject the null in the case of the first differences of y1,y2,y3,y4,y5,y6,y7,y8,y9.y10, we can conclude that the yi is from I(1) (i=1,2,3.....)


Again, we can conclude that the yi` is integrated of order 1, as we can reject the null in the case of the first differences. 

As a result, both variables are of order 1, so in the next step we can check whether they are **cointegrated**.

To estimating the cointegrating vector, we will estimate the following model of standard OLS regression model of Engle Granger's method :
```{r}
model.coint <- lm(y4 ~ y8, data = TS2)
```

# 3. Let's examine the model summary:
```{r}
summary(model.coint)
```
# We can observe that y8 has high significance on standard OLS regression model of Engle Granger's method and if y8 increases by 1 unit  y4 increases by 0.500849 units in the long run.

Next, we have to test stationarity of residuals. We found that the residuals of the model(y4-y8) is white noise as null hypothesis is rejected.

```{r}
testdf(variable = residuals(model.coint), max.augmentations = 3)
```

##

The ADF test with no augmentations can be used its result is that non-stationarity of residuals is **STRONGLY REJECTED**, so residuals are **stationary**, which means that `yi` and `yi` are **cointegrated**.



Now, let's create first lags of variables and adding them to the dataset

```{r}
Y <- c('y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10')

for (i in 1:9) {
  for (j in (i + 1):10) {
    model.coint <- lm(formula = paste(Y[i], "~", Y[j]), data = TS2)
    testdf(variable = residuals(model.coint), max.augmentations = 3)
  }
}
```
We can able to find 4 pairs that are cointegrating.



Importing data from the `csv` file:
```{r}
TS <- read_csv("TSA_2024_project_data_1.csv")
```

Verification of the structure:
```{r}
TS %>% glimpse()
TS %>% head()
TS %>% tail()
```


```{r}
TS$date <- as.Date(TS$date, format = "%Y-%m-%d")
TS %>% glimpse()
```

Now, we can transform it into the `xts` object:
```{r}
TS <- xts(TS[, -1], TS$date)
```
```{r}
plot(TS[,c(4,8)],
     col = c("green", "red")

    )
```
                                 Plot of y8 and y4


```{r}
TS280 <- TS[index(TS) < as.Date("2022-03-08"), ]
tail(TS280)
```

## 4.Johansen cointegration test

We have already performed a univariate cointegration test of Engle and Granger. Our conclusions are that `y4` and `y8` variables are cointegrated of order ~CI(1,1). 

As an alternative, we will now perform a multivariate test of Johansen. We will assume the `K=6` lag structure.

         Johansen Trace test

```{r}
johan.test.trace <- 
  ca.jo(TS280[, c("y4", "y8")], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "trace",  # type of the test: trace or eigen
        K = 6,           # lag order of the series (levels) in the VAR
    ) 
summary(johan.test.trace) 
```

Let's find interpretation of the test results:
```{r }
cbind(summary(johan.test.trace)@teststat, summary(johan.test.trace)@cval)
```

Let's recall that if the test statistic is SMALLER than the critical value, we CANNOT reject the null. If the test statistic is LARGER than the critical value, we REJECT the null.

We start with testing the hypothesis that **r = 0**. The testing statistic is greater than the critical value, hence the null is rejected (at 5% level).

Next, we test that **r <= 1**. In this case, the testing statistic is lower that the critical value, hence the null is NOT rejected.

We can say  there is 1 cointegrating vector for pair y4 and y8

4 b. It is the first eigenvector (the first column):
```{r}
summary(johan.test.trace)@V
```

Weights W: 
```{r}
summary(johan.test.trace)@W
```



Let's apply the alternative variant of the test: Maximal Eigen Value Test

```{r}
johan.test.eigen <- 
  ca.jo(TS280[, c("y4", "y8")], # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "eigen",  # type of the test: trace or eigen
        K = 6,           # lag order of the series (levels) in the VAR
) 
summary(johan.test.eigen) 
```

The conclusions are the same: 1 cointegrating vector same as Trace test. The variant of the Johansen test does not have impact on the parameters of cointegrating vector.



## 5 The VECM model


The Trace and Maximal Eigen Value give the same cointegrating vector, so any of the two can be used to estimate VECM.

Estimation of VECM Model

```{r}
TS280.vec4 <- cajorls(johan.test.eigen, # defined specification
                        r = 1) # number of cointegrating vectors
```
```{r}
TS280.vec4 %>% head(3)
```


```{r}
summary(TS280.vec4)
```

To print results we may refer to the element called `$rlm` and produce its summary with significance tests.

```{r}
str(TS280.vec4)
```

#6. Interpretation of parameters of  VECM  model.
```{r}
summary(TS280.vec4$rlm)
```


##6a. This is the F-test for y4 on y8 for joint significance test where null hypothesis is given below
$$H_0 : \alpha_{11}=\gamma_{11}=\gamma_{12} =\gamma_{13}=\gamma_{14}=\gamma_{15}=0 $$
##The p-value for joint significance test is less than 0.05. Hence the null hypothesis is rejected and the parameters/coefficients  are jointly significant. 
##We can observe that p-values of each parameters/coefficients are less than 0.05 and hence each parameters has high significance for y4 on y8.
$$H_0 : \alpha_{11}=0 $$
##Here the p-value is less than 0.05 and the coefficient of ect1 is non-zero and hence it is highly significant and thus the error correction  mechanism exists.

##The sign of the estimated  coefficient/ parameter of ect1 is negative for y4 on y8

##6b. This is the F-test for y8 on y4 for joint significance test where null hypothesis is given below
$$H_0 : \alpha_{22}=\gamma_{21}=\gamma_{22} =\gamma_{23}=\gamma_{24}=\gamma_{25}=0 $$
##The p-value for joint significance test is less than 0.05. Hence the null hypothesis is rejected and the parameters/coefficients  are jointly significant.

##We can observe that p-values of each parameters/coefficients are not  less than 0.05 individually and hence each parameters is not  significant for y8 on y4.

$$H_0 : \alpha_{21}=0 $$
##Here the p-value is less than 0.1 and the coefficient is not zero and  significant at 10% level and the error correction mechanism exists.
##The sign  of the coefficient of ect1 is expected to be positive but comes as negative for y8 on y4.





The VECM model  is given by where p= 5 for VECM:

$$\Delta\boldsymbol{y}_t = \Pi\boldsymbol{y}_{t-1} + \Gamma_1\Delta\boldsymbol{y}_{t-1} + ... + \Gamma_5\Delta\boldsymbol{y}_{t-5} + \boldsymbol{\varepsilon}_t$$

The `ect1` elements in both equations contain the adjustment coefficients ($\alpha_{11}$ and $\alpha_{21}$), which are the result of the decomposition of the $\Pi$ matrix:

$$
\boldsymbol{\Pi}=\alpha\beta'=\left[
	\begin{array}{ccc}
	\alpha_{11} \\
	\alpha_{21} \\
	\end{array} 
\right]
\left[
	\begin{array}{cccc}
\beta_{11} \quad \beta_{21} \\
 \end{array}
\right]
$$



We can extract the cointegrating vector in the following way:
```{r}
TS280.vec4$beta
```

##7 We can reparametrize the VEC model into VAR model:
```{r}
TS280.vec4.asVAR <- vec2var(johan.test.eigen, r = 1)
```

Lets see the result:
```{r}
TS280.vec4.asVAR
```

## 8 Based on the reparametrized model, we can calculate and plot Impulse Response Functions:

```{r}
 plot(irf(TS280.vec4.asVAR, n.ahead = 20))
```
 # We can observe the shock from y4 impact on all variables and after 20 time periods the response of the shock disappears and shock from y8 or from y4 has more impact on y4
 
 
## 9 We can also perform for the  Forecast Error Variance Decomposition:
```{r}
plot(fevd(TS280.vec4.asVAR, n.ahead = 20))
```
 We can observe that the variance of forecast errors of y4 can be explained by the shocks to each explanatory varibles y4 and y8 for the time ahead as s= 19 .
 
 


## 10 We check  if model residuals are autocorrelated.

Residuals can be extracted  from the VAR reparametrized model.

```{r}
tail(residuals(TS280.vec4.asVAR))
serial.test(TS280.vec4.asVAR)
```
 Null hypothesis is the residuals of the VAR model are no autocorrelated and the p-value is greater than 0.05 and 0.1 
The null is failed to be rejected and thus there is no autocorrelation of residuals of y4 and  y8 of the VAR model(6)
We have taken lag p=6 for VAR becuase for  p<6 the residuals of y4 and y8 are autocorrelated.



##11 You can see the ACF and PACF functions by plotting the results of the `serial.test()`
```{r fig.width=12, fig.height = 12}
plot(serial.test(TS280.vec4.asVAR))
```



##11  Histograms of residuals for the VAR Model

```{r}
TS280.vec4.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of y8`)) +
  geom_histogram(aes(y =..density..),
                 colour = "red", 
                 fill = "green") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(TS280.vec4.asVAR)[, 1]), 
                            sd = sd(residuals(TS280.vec4.asVAR)[, 1]))) +
  theme_bw() + 
  labs(
    title = "Density of y8 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

```{r}
TS280.vec4.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of y4`)) +
  geom_histogram(aes(y =..density..),
                 colour = "blue", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(TS280.vec4.asVAR)[, 2]), 
                            sd = sd(residuals(TS280.vec4.asVAR)[, 2]))) +
  theme_bw() + 
  labs(
    title = "Density of y4 residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

##We have checked  normality of residuals by applying the Jarque-Bera (JB) test. 
```{r}
normality.test(TS280.vec4.asVAR)
```
##P-value for JB normality test is greater than 0.05 and hence the null about normality is failed to be rejected and the residuals are normally distributed 
##There is no skewness as the null hypothesis about no skewness is failed to be rejected and same as for the Kurtosis.
##Hence the residuals for VAR model(6) are normally distributed


## 12 Forecasting based on the VECM 

Now we will calculate 20 days forecasts for y4 and y8` variables.






Forecasting the model based on VECM
```{r}
TS.vec4.fore <- 
  predict(
    vec2var(
      johan.test.eigen, 
      r = 1),     # no of cointegrating vectors 
    n.ahead = 20, # forecast horizon
    ci = 0.95)    # confidence level for intervals
```



VEC forecasts for `y4`:
```{r}
TS.vec4.fore$fcst$y4
```

VEC forecasts for `y8`:
```{r}
TS.vec4.fore$fcst$y8
```


```{r}
tail(index(TS), 20)
y4_forecast <- xts(TS.vec4.fore$fcst$y4[,-4], 
                    # we exclude the last column with CI
                    tail(index(TS), 20))
```

For y4 forecast:
```{r}
names(y4_forecast) <- c("y4_fore", "y4_lower", "y4_upper")
```

 For `y8` forecasts:
```{r}
y8_forecast <- xts(TS.vec4.fore$fcst$y8[, -4],
                    # we exclude the last column with CI
                    tail(index(TS), 20))
names(y8_forecast) <- c("y8_fore", "y8_lower", "y8_upper")
```

Now, we can merge the data together:
```{r}
TS <- merge(TS, 
                 y4_forecast,
                 y8_forecast)
```

## Lets compare the forecasted and real data on the plot.

```{r}
plot(TS[index(TS) > as.Date("2022-03-07"), c("y4", "y4_fore",
                        "y4_lower", "y4_upper")], 
  
     main = "20 days forecast of energy y4",
     col = c("black", "blue", "red", "red"))
plot(TS[index(TS) > as.Date("2022-03-07"), c("y8", "y8_fore",
                        "y8_lower", "y8_upper")], 

     main = "20 days forecast of energy y8",
     col = c("black", "blue", "red", "red"))
```
Here we get the forecasting price of y8 and y4 on 20 time periods and the original and projected observation are lying within the upper and lower boundaries. HERE the blue represents the projected/forecast and the blue represents the original observation.

## Exercises 13


 We have Calculated forecast accuracy measures (MAE, MSE, MAPE, AMAPE) based on VECM model and compared it with the accuracy of the forecast based on VECM model

```{r}
TS$mae.y4   <-  abs(TS$y4 - TS$y4_fore)
TS$mse.y4   <-  (TS$y4 - TS$y4_fore) ^ 2
TS$mape.y4  <-  abs((TS$y4 - TS$y4_fore)/TS$y4)
TS$amape.y4 <-  abs((TS$y4 - TS$y4_fore) / 
                            (TS$y4 + TS$y4_fore))

TS$mae.y8   <-  abs(TS$y8 - TS$y8_fore)
TS$mse.y8   <-  (TS$y8 -TS$y8_fore) ^ 2
TS$mape.y8  <-  abs((TS$y8 - TS$y8_fore)/TS$y8)
TS$amape.y8 <-  abs((TS$y8 - TS$y8_fore) / 
                            (TS$y8 + TS$y8_fore))
```
```{r}
tail(TS,20)
```

```{r}
colMeans(TS[, 17:24], na.rm = TRUE)
```



### Interpretation of Errors:


- **MAE for y4**: 0.986
  - On average, the absolute error in predictions for `y4` is 0.986 units. This indicates the typical size of the error in your predictions.
  
- **MSE for y4**: 1.399
  - The mean squared error for `y4` is 1.399. This value is higher than the MAE, suggesting that there are some larger errors that are being squared, thus increasing the overall error value.

- **MAPE for y4**: 0.0056 (or 0.56%)
  - The mean absolute percentage error for `y4` is 0.56%. This indicates that, on average, the predictions for `y4` deviate from the actual values by 0.56% of the actual values.

- **AMAPE for y4**: 0.0028 (or 0.28%)
  - The adjusted mean absolute percentage error for `y4` is 0.28%. This metric provides a different perspective on the error, accounting for the scale of the actual and predicted values.

- **MAE for y8**: 1.938
  - On average, the absolute error in predictions for `y8` is 1.938 units, which is significantly higher than that for `y4`.

- **MSE for y8**: 5.230
  - The mean squared error for `y8` is 5.230. This is much higher than the MSE for `y4`, suggesting the presence of larger errors in the predictions for `y8`.

- **MAPE for y8**: 0.0131 (or 1.31%)
  - The mean absolute percentage error for `y8` is 1.31%. This indicates that, on average, the predictions for `y8` deviate from the actual values by 1.31% of the actual values.

- **AMAPE for y8**: 0.0066 (or 0.66%)
  - The adjusted mean absolute percentage error for `y8` is 0.66%. Again, this provides a sense of the prediction error relative to the combined scale of actual and predicted values.

### Summary:

- The errors for `y4` are generally lower than those for `y8`, indicating better prediction accuracy for `y4`.
- Both MSE and MAE are higher for `y8` than for `y4`, with the difference in MSE being particularly pronounced, suggesting that the predictions for `y8` have larger errors.
- The MAPE and AMAPE for `y4` are significantly lower than for `y8`, indicating more accurate percentage-based predictions for `y4`.

Overall, these metrics suggest that the model's predictions for `y4` are more accurate and consistent than those for `y8`.





